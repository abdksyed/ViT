"""
---
title: Vision Transformer (ViT)
summary: >
 A PyTorch implementation/tutorial of the paper
 "An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale"
---

#  Vision Transformer (ViT)
This is a [PyTorch](https://pytorch.org) implementation of the paper
[An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://papers.labml.ai/paper/2010.11929).

Vision transformer applies a pure transformer to images without any convolution layers.
They split the image into patches and apply a transformer on patch embeddings.
[Patch embeddings](#PatchEmbeddings) are generated by applying a simple linear transformation
to the flattened pixel values of the patch.
Then a standard transformer encoder is fed with the patch embeddings, along with a
classification token `[CLS]`.
The encoding on the `[CLS]` token is used to classify the image with an MLP.
When feeding the transformer with the patches, learned positional embeddings are
added to the patch embeddings, because the patch embeddings do not have any information
about where that patch is from.
The positional embeddings are a set of vectors for each patch location that get trained
with gradient descent along with other parameters.
ViTs perform well when they are pre-trained on large datasets.
The paper suggests pre-training them with an MLP classification head and
then using a single linear layer when fine-tuning.
The paper beats SOTA with a ViT pre-trained on a 300 million image dataset.
They also use higher resolution images during inference while keeping the
patch size the same.
The positional embeddings for new patch locations are calculated by interpolating
learning positional embeddings.

This work is inspired and also adopted from the great work of
[LabML ViT Implementaion](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/vit/__init__.py)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

from labml_helpers.module import Module
from labml_nn.transformers import TransformerLayer
from labml_nn.utils import clone_module_list

import collections.abc

def to_2tuple(x):
    """
    Converts the single value 'x' to a tuple of ('x', 'x')
    """
    if isinstance(x, collections.abc.Iterable):
        return x
    return (x, x)

# Variables for Image Size and Patch Size
img_size = 224
patch_size = 16

# img_size -> (224,224), path_size -> (16,16)
img_size = to_2tuple(img_size)
patch_size = to_2tuple(patch_size)

# Batch of Images.
# Each image is a tensor of shape (3, 224, 224)
x = torch.rand((32, 3, 224, 224))

# Converitng Image to Patch Embeddings.
# We create a convolution layer with a kernel size and and stride length equal to patch size.
# This is equivalent to splitting the image into patches and doing a linear
# transformation on each patch.
# y -> (32, 768, 14, 14)
proj = nn.Conv2d(3, 768, 16, 16)
y = proj(x)

# After Flattening & Transpose y -> (32, 196, 768) 
y = y.flatten(2).transpose(1, 2)


class PatchEmbeddings(nn.Module):
    """
    <a id="PatchEmbeddings">
    ## Get patch embeddings
    </a>

    The paper splits the image into patches of equal size and do a linear transformation
    on the flattened pixels for each patch.

    We implement the same thing through a convolution layer, because it's simpler to implement.
    
    This class implementation is the combined of the above steps.
    """

    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):
        """
        Arguments:

        * image_size: The size of the image to split into patches.
        * patch_size: The size of the patches to split the image into.
        * num_channels: The number of channels in the image.
        * embed_dim: The dimension of the embeddings.
        """
        super().__init__()

        # Converting image size, patch to a tuple if needed
        image_size = to_2tuple(image_size)
        patch_size = to_2tuple(patch_size)
        # Number of Patches in the Image
        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])
        #
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        # Image to Patch Embeddings(batch, embed_dim, num_patches, num_patches)
        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        """
        * `x` is the input image of shape `[batch_size, channels, height, width]`
        """
        # Get the shape.
        batch_size, num_channels, height, width = x.shape
        # Size Constraints to have proper division of Image.
        if height != self.image_size[0] or width != self.image_size[1]:
            raise ValueError(
                f"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]})."
            )
        # Apply convolution layer and Rearrange
        x = self.projection(x).flatten(2).transpose(1, 2)
        #
        return x


class ViTEmbeddings(nn.Module):
    """
    Construct the CLS token, position and patch embeddings.
    """
    def __init__(self, config):
        """
        Arguments:

        * config: A dictionary of configurations.
        """
        super().__init__()
        # Learnable Class Token.
        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))
        # Patches Embeddings
        self.patch_embeddings = PatchEmbeddings(
            image_size=config.image_size,
            patch_size=config.patch_size,
            num_channels=config.num_channels,
            embed_dim=config.hidden_size,
        )
        #
        num_patches = self.patch_embeddings.num_patches
        # Learnable Position Embeddings
        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))
        #
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, x):
        """
        * `x` is the input image of shape `[batch_size, channels, height, width]`
        """
        # Getting the batch size
        batch_size = x.shape[0]
        # Applying Patch Embeddings to Image
        embeddings = self.patch_embeddings(x)
        # Creating(Expanding) Class Token dynamically for entire Batch 
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        # Concatenating Class Token and Patch Embeddings
        embeddings = torch.cat((cls_tokens, embeddings), dim=1)
        # Adding Postional Embeddings to Patch Embeddings
        embeddings = embeddings + self.position_embeddings
        # Applying Dropout
        embeddings = self.dropout(embeddings)
        #
        return embeddings


# Can read full configuration file here: https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit/configuration_vit.py
class ViTConfig():
    """
    A class to store the configurations for the ViT model.
    """
    def __init__(
            self,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072,
            hidden_act="gelu",
            hidden_dropout_prob=0.0,
            attention_probs_dropout_prob=0.0,
            initializer_range=0.02,
            layer_norm_eps=1e-12,
            is_encoder_decoder=False,
            image_size=224,
            patch_size=16,
            num_channels=3,
            **kwargs
        ):
        """
        Arguments:

        * `hidden_size`: Hidden size of the embeddings in encoder and decoder.
        * `num_hidden_layers`: Number of hidden layers in the encoder and decoder.
        * `num_attention_heads`: Number of attention heads in the encoder and decoder.
        * `intermediate_size`: The size of the "intermediate" (i.e., feed-forward) layer in the
            encoder and decoder.
        * `hidden_act`: The non-linear activation function (function or string) in the
            encoder. If string, "gelu", "relu" and "swish" are supported
        * `hidden_dropout_prob`: The dropout probabilitiy for all fully connected layers in the
            embeddings, encoder, and pooler.
        * `attention_probs_dropout_prob`: The dropout ratio for the attention probabilities.
        * `initializer_range`: The sttdev of the truncated_normal_initializer for
            initializing all weight matrices.
        * `layer_norm_eps`: The epsilon used by LayerNorm.
        * `is_encoder_decoder`: Whether the model is used as only encoder or encoder-decoder.
        * `image_size`: The input image size.
        * `patch_size`: The size of the patches in the image.
        * `num_channels`: The number of channels in the input image.
        """
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.initializer_range = initializer_range
        self.layer_norm_eps = layer_norm_eps

        self.image_size = image_size
        self.patch_size = patch_size
        self.num_channels = num_channels







