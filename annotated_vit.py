"""
---
title: Vision Transformer (ViT)
summary: >
 A PyTorch implementation/tutorial of the paper
 "An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale"
---

#  Vision Transformer (ViT)
This is a [PyTorch](https://pytorch.org) implementation of the paper
[An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://papers.labml.ai/paper/2010.11929).

Vision transformer applies a pure transformer to images without any convolution layers.
They split the image into patches and apply a transformer on patch embeddings.
[Patch embeddings](#PatchEmbeddings) are generated by applying a simple linear transformation
to the flattened pixel values of the patch.
Then a standard transformer encoder is fed with the patch embeddings, along with a
classification token `[CLS]`.
The encoding on the `[CLS]` token is used to classify the image with an MLP.
When feeding the transformer with the patches, learned positional embeddings are
added to the patch embeddings, because the patch embeddings do not have any information
about where that patch is from.
The positional embeddings are a set of vectors for each patch location that get trained
with gradient descent along with other parameters.
ViTs perform well when they are pre-trained on large datasets.
The paper suggests pre-training them with an MLP classification head and
then using a single linear layer when fine-tuning.
The paper beats SOTA with a ViT pre-trained on a 300 million image dataset.
They also use higher resolution images during inference while keeping the
patch size the same.
The positional embeddings for new patch locations are calculated by interpolating
learning positional embeddings.

This work is inspired and also adopted from the great work of
[LabML ViT Implementaion](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/transformers/vit/__init__.py)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

from labml_helpers.module import Module
from labml_nn.transformers import TransformerLayer
from labml_nn.utils import clone_module_list

import collections.abc

def to_2tuple(x):
    """
    Converts the single value 'x' to a tuple of ('x', 'x')
    """
    if isinstance(x, collections.abc.Iterable):
        return x
    return (x, x)

# Variables for Image Size and Patch Size
img_size = 224
patch_size = 16

# img_size -> (224,224), path_size -> (16,16)
img_size = to_2tuple(img_size)
patch_size = to_2tuple(patch_size)

# Batch of Images.
# Each image is a tensor of shape (3, 224, 224)
x = torch.rand((32, 3, 224, 224))

# Converitng Image to Patch Embeddings.
# We create a convolution layer with a kernel size and and stride length equal to patch size.
# This is equivalent to splitting the image into patches and doing a linear
# transformation on each patch.
# y -> (32, 768, 14, 14)
proj = nn.Conv2d(3, 768, 16, 16)
y = proj(x)

# After Flattening & Transpose y -> (32, 196, 768) 
y = y.flatten(2).transpose(1, 2)


class PatchEmbeddings(Module):
    """
    <a id="PatchEmbeddings">
    ## Get patch embeddings
    </a>

    The paper splits the image into patches of equal size and do a linear transformation
    on the flattened pixels for each patch.

    We implement the same thing through a convolution layer, because it's simpler to implement.
    
    This class implementation is the combined of the above steps.
    """

    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):
        """
        Arguments:

        * image_size: The size of the image to split into patches.
        * patch_size: The size of the patches to split the image into.
        * num_channels: The number of channels in the image.
        * embed_dim: The dimension of the embeddings.
        """
        super().__init__()

        # Converting image size, patch to a tuple if needed
        image_size = to_2tuple(image_size)
        patch_size = to_2tuple(patch_size)
        # Number of Patches in the Image
        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])
        #
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        # Image to Patch Embeddings(batch, embed_dim, num_patches, num_patches)
        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        """
        * `x` is the input image of shape `[batch_size, channels, height, width]`
        """
        # Get the shape.
        batch_size, num_channels, height, width = x.shape
        # Size Constraints to have proper division of Image.
        if height != self.image_size[0] or width != self.image_size[1]:
            raise ValueError(
                f"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]})."
            )
        # Apply convolution layer and Rearrange
        x = self.projection(x).flatten(2).transpose(1, 2)
        #
        return x


class ViTEmbeddings(Module):
    """
    Construct the CLS token, position and patch embeddings.
    """
    def __init__(self, config):
        """
        Arguments:

        * config: A dictionary of configurations.
        """
        super().__init__()
        # Learnable Class Token.
        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))
        # Patches Embeddings
        self.patch_embeddings = PatchEmbeddings(
            image_size=config.image_size,
            patch_size=config.patch_size,
            num_channels=config.num_channels,
            embed_dim=config.hidden_size,
        )
        #
        num_patches = self.patch_embeddings.num_patches
        # Learnable Position Embeddings
        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))
        #
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, x):
        """
        * `x` is the input image of shape `[batch_size, channels, height, width]`
        """
        # Getting the batch size
        batch_size = x.shape[0]
        # Applying Patch Embeddings to Image
        embeddings = self.patch_embeddings(x)
        # Creating(Expanding) Class Token dynamically for entire Batch 
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        # Concatenating Class Token and Patch Embeddings
        embeddings = torch.cat((cls_tokens, embeddings), dim=1)
        # Adding Postional Embeddings to Patch Embeddings
        embeddings = embeddings + self.position_embeddings
        # Applying Dropout
        embeddings = self.dropout(embeddings)
        #
        return embeddings


# Can read full configuration file here: https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit/configuration_vit.py
class ViTConfig():
    """
    A class to store the configurations for the ViT model.
    """
    def __init__(
            self,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072,
            hidden_act="gelu",
            hidden_dropout_prob=0.0,
            attention_probs_dropout_prob=0.0,
            initializer_range=0.02,
            layer_norm_eps=1e-12,
            is_encoder_decoder=False,
            image_size=224,
            patch_size=16,
            num_channels=3,
            **kwargs
        ):
        """
        Arguments:

        * `hidden_size`: Hidden size of the embeddings in encoder and decoder.
        * `num_hidden_layers`: Number of hidden layers in the encoder and decoder.
        * `num_attention_heads`: Number of attention heads in the encoder and decoder.
        * `intermediate_size`: The size of the "intermediate" (i.e., feed-forward) layer in the
            encoder and decoder.
        * `hidden_act`: The non-linear activation function (function or string) in the
            encoder. If string, "gelu", "relu" and "swish" are supported
        * `hidden_dropout_prob`: The dropout probabilitiy for all fully connected layers in the
            embeddings, encoder, and pooler.
        * `attention_probs_dropout_prob`: The dropout ratio for the attention probabilities.
        * `initializer_range`: The sttdev of the truncated_normal_initializer for
            initializing all weight matrices.
        * `layer_norm_eps`: The epsilon used by LayerNorm.
        * `is_encoder_decoder`: Whether the model is used as only encoder or encoder-decoder.
        * `image_size`: The input image size.
        * `patch_size`: The size of the patches in the image.
        * `num_channels`: The number of channels in the input image.
        """
        self.hidden_size = hidden_size
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads
        self.intermediate_size = intermediate_size
        self.hidden_act = hidden_act
        self.hidden_dropout_prob = hidden_dropout_prob
        self.attention_probs_dropout_prob = attention_probs_dropout_prob
        self.initializer_range = initializer_range
        self.layer_norm_eps = layer_norm_eps

        self.image_size = image_size
        self.patch_size = patch_size
        self.num_channels = num_channels


# {'attention_probs_dropout_prob': 0.0,
# 'hidden_act': 'gelu',
# 'hidden_dropout_prob': 0.0,
# 'hidden_size': 768,
# 'image_size': 224,
# 'initializer_range': 0.02,
# 'intermediate_size': 3072,
# 'layer_norm_eps': 1e-12,
# 'num_attention_heads': 12,
# 'num_channels': 3,
# 'num_hidden_layers': 12,
# 'patch_size': 16}
configuration = ViTConfig()
print(vars(configuration))

# Math for sqrt
import math
# The heart of Transformer - Self Attention Mechanism
class ViTSelfAttention(Module):
    """
    The class performs self attention mechanism on the input sequence.

    Self Attention is a technique where we calculate attention weights for all the tokens(patches) in the
    sequence and treat the attention weights as a mask over the tokens(patches) in the sequence. This allows
    us to only pay attention to certain tokens(patches).
    """
    def __init__(self, config):
        """
        Arguments:

        * `config`: A class with configuration for the self attention mechanism.
        """
        super().__init__()
        # If number of attention heads is not a divisor of hidden size embeddings, raise Value Error
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                f"The hidden size {config.hidden_size,} is not a multiple of the number of attention "
                f"heads {config.num_attention_heads}."
            )
        #
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        # Query, Key and Value are generated using the Patch Embedding sent to 3 different Linear Layers.
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        # 
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        """
        Converting the each of key, query and value to multi-head.

        * `x`: A tensor with shape [batch_size, seq_len, all_head_size]
        """
        # (32,768) + (12,64) -> (32,768,12,64)
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        # (32,768,196) -> (32,768,12,196)
        x = x.view(*new_x_shape)
        # (32,768,12,64) -> (32,12,768,196)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, head_mask=None, output_attentions=False):
        """
        * `hidden_states`: A tensors of shape `[batch_size, seq_len, hidden_size]`.
        * `head_mask`: A masking tensor of shape `[num_heads, num_hidden_layers]`.
        * `output_attentions`: Whether to output attentions weights.
        """
        # Generating Key, Query and Value using the hidden_states 
        key_layer = self.transpose_for_scores(self.key(hidden_states))
        value_layer = self.transpose_for_scores(self.value(hidden_states))
        query_layer = self.transpose_for_scores(self.query(hidden_states))

        # Take the dot product between "query" and "key" to get the raw attention scores.
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)

        # Normalize the attention scores to probabilities.
        attention_probs = nn.Softmax(dim=-1)(attention_scores)

        # This is actually dropping out entire tokens to attend to, which might
        # seem a bit unusual, but is taken from the original Transformer paper.
        attention_probs = self.dropout(attention_probs)

        # Mask heads if we want to
        if head_mask is not None:
            attention_probs = attention_probs * head_mask

        # Take the weighted sum of the values to attention.
        context_layer = torch.matmul(attention_probs, value_layer)

        # context_layer = context_layer.permute(0, 2, 1, 3)
        # new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        # context_layer.view(*new_context_layer_shape) will throw an error
        # (at least one dimension spans across two contiguous subspaces)
        # This is where the concept of contiguous comes in. x is contiguous
        # but y is not because its memory layout is different to that of a
        # tensor of same shape made from scratch. Note that the word
        # "contiguous" is a bit misleading because it's not that the
        # content of the tensor is spread out around disconnected blocks
        # of memory. Here bytes are still allocated in one block of memory
        # but the order of the elements is different!
        # When we call contiguous(), it actually makes a copy of the tensor
        # such that the order of its elements in memory is the same as if it
        # had been created from scratch with the same data.
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)

        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)

        return outputs




